{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cuda device  4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# GPU 할당 변경하기\n",
    "GPU_NUM = 4  # 원하는 GPU 번호 입력\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "print ('Current cuda device ', torch.cuda.current_device()) # check\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.load('data_label/x_data.npy').reshape(2210,1,257,382)\n",
    "y_data = np.load('data_label/y_data.npy').reshape(2210,1,257,382)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.from_numpy( x_data[:1000,:,:,:] ).float().to(device)\n",
    "y_data = torch.from_numpy( y_data[:1000,:,:,:] ).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 2210\n",
    "#valid_size = \n",
    "\n",
    "full_dataset = TensorDataset(x_data, y_data)\n",
    "\n",
    "#train_data, valid_data = torch.utils.data.random_split(full_dataset, [train_size, valid_size])\n",
    "train_dataset = DataLoader( dataset=full_dataset, batch_size = 5, shuffle=True, drop_last=True)\n",
    "#valid_dataset = DataLoader( dataset=valid_data, batch_size = 25, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def conv_trans_block( in_dim, out_dim, act_fn ):\n",
    "    model = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_dim, out_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        # (H-1)*2 -2*1 + 1*(3-1) +1 +1 = 2*H\n",
    "        nn.BatchNorm2d(out_dim),\n",
    "        act_fn)\n",
    "    return model\n",
    "\n",
    "\n",
    "# (H,W) -> (H/2, W/2)\n",
    "def maxpool():\n",
    "    pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "    return pool\n",
    "\n",
    "\n",
    "def conv_block( in_dim, out_dim, act_fn ):\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(in_dim, out_dim, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(out_dim),\n",
    "        act_fn,\n",
    "        #nn.Conv2d(out_dim, out_dim, kernel_size=3,  padding=1),\n",
    "        #nn.BatchNorm2d(out_dim),\n",
    "        )\n",
    "    return model\n",
    "\n",
    "\n",
    "def conv_input( in_dim, out_dim, act_fn ):\n",
    "    model = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, out_dim, kernel_size=(3,2), padding=1), # H, W+1\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            act_fn,\n",
    "            #nn.init.xavier_uniform_(act_fn.weight),\n",
    "            nn.Conv2d(out_dim, out_dim, kernel_size=(2,2),  padding=(0,1)), # H-1, W+1\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            )\n",
    "    return model\n",
    "\n",
    "\n",
    "def conv_output( in_dim, out_dim, act_fn ):\n",
    "    model = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, out_dim, kernel_size=(3,4), padding=(1,1)), # H, W-1\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            act_fn,\n",
    "            nn.Conv2d(out_dim, out_dim, kernel_size=(2,4),  padding=(1,1)), # H+1, W-1\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetGenerator(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_filter):\n",
    "        super(UnetGenerator, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_filter = num_filter\n",
    "        act_fn = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        \n",
    "        self.down_1 = conv_input(self.in_dim, self.num_filter*1, act_fn) #(257,382) -> (1,256,384)\n",
    "        self.pool_1 = maxpool() #(1,256,384) -> (1,128,192)\n",
    "        self.down_2 = conv_block(self.num_filter*1, self.num_filter*2, act_fn) \n",
    "        self.pool_2 = maxpool() #(1,128,192) -> (2,64,96)\n",
    "        self.down_3 = conv_block(self.num_filter*2, self.num_filter*4, act_fn)\n",
    "        self.pool_3 = maxpool() #(2,64,96) -> (4,32,48)\n",
    "        self.down_4 = conv_block(self.num_filter*4, self.num_filter*8, act_fn)\n",
    "        self.pool_4 = maxpool() #(4,32,48) -> (8,16,24)\n",
    "        self.bridge = conv_block(self.num_filter*8, self.num_filter*16, act_fn) #(8,32,48) -> (16,16,24)\n",
    "        \n",
    "    \n",
    "        self.trans_1 = conv_trans_block(self.num_filter*16, self.num_filter*8, act_fn)\n",
    "        self.up_1 = conv_block(self.num_filter*16, self.num_filter*8, act_fn) #(16,16,24) -> (8,32,48)    \n",
    "        self.trans_2 = conv_trans_block(self.num_filter*8, self.num_filter*4, act_fn)\n",
    "        self.up_2 = conv_block(self.num_filter*8, self.num_filter*4, act_fn) #(8,32,48)  -> (4,64,96)\n",
    "        self.trans_3 = conv_trans_block(self.num_filter*4, self.num_filter*2, act_fn)\n",
    "        self.up_3 = conv_block(self.num_filter*4, self.num_filter*2, act_fn) #(4,64,96)  -> (2,128,192)\n",
    "        self.trans_4 = conv_trans_block(self.num_filter*2, self.num_filter*1, act_fn)\n",
    "        self.up_4 = conv_output(self.num_filter*2, self.num_filter*1, act_fn) #(2,128,192) -> (1,257,382)\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "                nn.Conv2d(self.num_filter, self.out_dim, kernel_size=3, padding=1),\n",
    "                nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "\n",
    "        #(N,1,257,382)\n",
    "        down_1 = self.down_1(input)  #(N,1*C,256,384)\n",
    "        pool_1 = self.pool_1(down_1) #(N,1*C,128,192)        \n",
    "        down_2 = self.down_2(pool_1) \n",
    "        pool_2 = self.pool_2(down_2) #(N,2*C,64,96)\n",
    "        down_3 = self.down_3(pool_2)\n",
    "        pool_3 = self.pool_3(down_3) #(N,4*C,32,48)\n",
    "        down_4 = self.down_4(pool_3)\n",
    "        pool_4 = self.pool_4(down_4) #(N,8*C,16,24)\n",
    "\n",
    "        bridge = self.bridge(pool_4) #(N,16*C,16,24)\n",
    "\n",
    "        trans_1  = self.trans_1(bridge) #(N,8*C,32,48)\n",
    "        concat_1 = torch.cat([trans_1, down_4], dim=1) #(N,16*C,32,48)\n",
    "        up_1     = self.up_1(concat_1) #(N,8*C,32,48)\n",
    "        trans_2  = self.trans_2(up_1)  #(N,4*C,64,96)\n",
    "        concat_2 = torch.cat([trans_2, down_3], dim=1) #(N,8*C,64,96)\n",
    "        up_2     = self.up_2(concat_2) #(N,4*C,64,96)\n",
    "        trans_3  = self.trans_3(up_2)  #(N,2*C,128,192)\n",
    "        concat_3 = torch.cat([trans_3, down_2], dim=1)  #(N,4*C,128,192)\n",
    "        up_3     = self.up_3(concat_3) #(N,2*C,128,192)\n",
    "        trans_4  = self.trans_4(up_3)  #(N,C,256,384)\n",
    "        concat_4 = torch.cat([trans_4, down_1], dim=1)  #(N,2*C,256,384)\n",
    "        up_4     = self.up_4(concat_4) #(N,C,257,382)\n",
    "\n",
    "        out = self.out(up_4)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim=1\n",
    "out_dim=1\n",
    "num_filter=16\n",
    "epoch=100\n",
    "lr=0.0001\n",
    "\n",
    "\n",
    "\n",
    "model = UnetGenerator(in_dim=in_dim, out_dim=out_dim, num_filter=num_filter)\n",
    "model = nn.DataParallel(model, device_ids=[4,5,0,1])\n",
    "model = model.cuda()\n",
    "\n",
    "loss_func = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "noise_pred = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    s = time.time()\n",
    "    for (data,label) in train_dataset:\n",
    "        (data, label) = (data.to(device), label.to(device))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = loss_func(output,label)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        noise_pred.append(output.data)\n",
    "    \n",
    "    print('epoch: ',i+1,',   loss:',loss.item(), ',   time: ',time.time()-s)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 tf-v2.2",
   "language": "python",
   "name": "kdh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
